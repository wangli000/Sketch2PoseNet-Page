<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/kbrodt/sketch2pose">
            Sketch2Pose
          </a>
          <!-- <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wangli000.github.io/">Li Wang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yiyuzhuang.github.io/home/">Yiyu Zhuang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yanwen-w.github.io/">Yanwen Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=8hZIngIAAAAJ&hl=zh-CN">Xun Cao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ericguo5513.github.io/">Chuan Guo</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/xinxinzuohome/">Xinxin Zuo</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="http://zhuhao.cc">Hao Zhu</a><sup>1✉</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Nanjing University, </span>
            <span class="author-block"><sup>2</sup>Snap Research, </span>
            <span class="author-block"><sup>3</sup>Concordia University</span>
          </div>

          <div class="is-size-6 publication-authors">
            ✉ Corresponding author
          </div>

          <div style="font-size: 1.5em; margin-top: 10px; font-weight: bold; text-align: center;">
            SIGGRAPH Asia 2025
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2510.26196"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.26196"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wangli000/Sketch2PoseNet"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Sketch2PoseNet</span> can efficiently predict 3D human poses from sketches.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D human pose estimation from sketches has broad applications in computer animation and film production.
            Unlike traditional human pose estimation, this task presents unique challenges due to the abstract and
            disproportionate nature of sketches. Previous sketch-to-pose methods, constrained by the lack of large-scale
            sketch-3D pose annotations, primarily relied on optimization with heuristic rules—an approach that is both
            time-consuming and limited in generalizability. To address these challenges, we propose a novel approach
            leveraging a "learn from synthesis" strategy. First, a diffusion model is trained to synthesize sketch images
            from 2D poses projected from 3D human poses, mimicking disproportionate human structures in sketches.
            This process enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k accurate sketch-3D
            pose annotation pairs across various sketch styles. Building on this synthetic dataset, we introduce an end-to-end
            data-driven framework for estimating human poses and shapes from diverse sketch styles. Our framework combines existing
            2D pose detectors and generative diffusion priors for sketch feature extraction with a feed-forward neural network for
            efficient 2D pose estimation. Multiple heuristic loss functions are incorporated to guarantee geometric coherence between
            the derived 3D poses and the detected 2D poses while preserving accurate self-contacts. Qualitative, quantitative,
            and subjective evaluations collectively show that our model substantially surpasses previous ones in both estimation
            accuracy and speed for sketch-to-pose tasks.
          </p>
        </div>
      </div>
    </div>

    <!-- Video -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe
            src="https://www.youtube.com/embed/nS4O_EXtmXg?rel=0&amp;showinfo=0"
            frameborder="0"
            allow="autoplay; encrypted-media"
            allowfullscreen>
          </iframe>
        </div>
      </div>
    </div>

    <!-- Method -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <figure class="image" style="margin-top:1rem;">
          <img src="./static/images/fig_pipeline.jpg" alt="Method pipeline" style="width:100%;height:auto;">
        </figure>
        <p class="content has-text-justified" style="margin-top:0.75rem;">
          <strong>Overall Method.</strong> Given a sketch image as input, the network predicts 3D human poses represented
          by SMPL parameters. The overall network consists of three modules: the 2D guidance extractor (i),
          the sketch feature extractor (ii), and the SMPL regressor (iii).
        </p>
      </div>
    </div>

    <!-- Dataset Pipeline -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset Pipeline</h2>
        <figure class="image" style="margin-top:1rem;">
          <img src="./static/images/fig_data_gen.jpg" alt="Dataset pipeline" style="width:100%;height:auto;">
        </figure>
        <p class="content has-text-justified" style="margin-top:0.75rem;">
          <strong>Dataset Creating Pipeline.</strong> Three stages are involved: (i) generating diverse 3D poses (as SMPL); 
            (ii) adding random biases to bone lengths and projecting to 2D poses; 
            (iii) generating diverse text guidance; (iv) training a text-conditioned image generator for sketch synthesis.
        </p>
      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{wang2025sketch2posenet,
  author    = {Li Wang, Yiyu Zhuang, Yanwen Wang, Xun Cao, Chuan Guo, Xinxin Zuo, Hao Zhu},
  title     = {Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction},
  journal   = {SIGGRAPH Asia},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
